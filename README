Homepage:
    http://www.steve.org.uk/Software/slaughter/

Mercurial Repository:
    http://slaughter.repository.steve.org.uk/




Slaughter
---------

The goal of this project is to have a lightweight system which will
allow the control and manipulation of multiple systems.

The tasks we support are pretty basic, but they include:

   1.  Replacing a file, literally.
   2.  Replacing a file, expanding content within it.
   3.  Appending lines to files, or commenting out lines.
   4.  Running system commands.
   5.  Testing for the existence of local users, and fetching their details.
   6.  Checking disk space, and mount-points.
   7.  Sending alerts via email.




Overview
--------

We assume that we have a "server" somewhere.  This server is designed to
allow files and policies to be retrieved from it.

The actual form of the server loosely specified, as slaughter allows different
transport mechanisms to be used.  Currently there are four supported transports
for fetching files:

  * A remote HTTP server.
  * A remote rsync server.
  * A remote GIT server.
  * A remote Mercurial server.

Each client which is running slaughter will connect to the server, using
the appropriate mechanism, and download policies.  These policies describe
the actions that must be carried out upon the local system.

Policies (and also files) are pulled by the client, meaning there is no
central server which is in charge of initiating transactins, or connections.

The attraction of client-pull is that there is no need to maintain state
on the central server, and each client can be trusted to schedule itself
via a cron-like system.  The potential downside is that you might fail to
notice if a client suddenly stops making appropriate requests.


Policies
--------

The list of tasks which each node should carry out is defined in a policy
file.  These policies are fetched from the server and may recursively
include other policies.

When a client node launches the first thing it will do is attempt to fetch
the policy "default.policy".

This default policy file can contain code in Perl, and the use of our
additional primitives.  It is expected that because the file default.policy
is fetched by all clients it should be used for house-keeping, and merely
include other policies.

For example this might be a sane default.policy file:

---

# actions to carry out globally
FetchPolicy  global.policy ;

# is there a per-client one?
FetchPolicy $fqdn.policy;

----

In this case the variable "fqdn" is expanded to the fully-qualified
domain-name of the requesting client - this is an example of one of the
many available defined variables clients may make use of in policy
files, or pure perl code.

If the hostname-specific policy file does not exist then it is merely
ignored.



Client Layout
-------------

To get started a client needs to have :

  a.  The slaughter-client package installed upon it.
  b.  The name of the server, and the transport to use against it, stored
     in the configuration file /etc/slaughter/slaughter.conf [*]

Once this is done cron may be used to ensure that /sbin/slaughter is invoked
upon a regular basis.  (Hourly is a good choice.)

You'll probably want to invoke it manually for the first few times as you're
putting together your policies.

  [*] - Alternatively you may specify these details on the command line.
        See the TRANSPORT file for some examples.



Server Layout
-------------

Regardless of the transport which is in use, and examples are available in
the included TRANSPORT file, slaughter does insist that the top-level of
the server contain two sub-directories.

For example using the HTTP-transport the webserver should be configured to
serve a tree which would look like this:

           /var/www/slaughter/
           /var/www/slaughter/policies/
           /var/www/slaughter/files/

Here we see there are two specific directories:

  policies/
    This is the location of the policies.
  files/
    This is the root of any files which are stored on the server to be fetched.

e.g. The request for http://$master/slaughter/policies/default.policy should
succeed.

In the interests of security it is probably wise to limit access to the
/slaughter/ location, denying access to clients you're not expecting to
pull from it.




In Depth Operation
------------------

The client node will first fetch the policy, which might contain
recursive calls to include other policies via "FetchPolicy path ;".

Once the (recursive) fetching has been completed the downloaded
content will be wrapped such that it becomes a locally executable file,
making use of the Slaughter.pm module (this module is where the primitives
are implemented and exported to the perl script).

Finally the locally written script will be made executable and
directly executed, before being removed.

The output of the script will be saved to a logfile.

Steve
--
